{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "##ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ import\n",
    "from glob import glob\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, colorstr, non_max_suppression, \\\n",
    "    apply_classifier, scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path, save_one_box\n",
    "from utils.plots import colors, plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socket created\n",
      "Socket bind complete\n",
      "Socket now listening\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-36f3336921c4>:42: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  data = np.fromstring(stringData, dtype='uint8')\n",
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "/home/piai/anaconda3/envs/yoloTest/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest1.jpg: 320x416 Done. (0.006s)\n",
      "Results saved to results\n",
      "Done. (0.010s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest2.jpg: 320x416 Done. (0.005s)\n",
      "Results saved to results\n",
      "Done. (0.010s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest3.jpg: 320x416 1 zzappagaetti, Done. (0.006s)\n",
      "Results saved to results\n",
      "Done. (0.013s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest4.jpg: 320x416 1 zzappagaetti, Done. (0.007s)\n",
      "Results saved to results\n",
      "Done. (0.013s)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-36f3336921c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecvall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mstringData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecvall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstringData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_COLOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import cv2\n",
    "import numpy as np\n",
    "# socketÏóêÏÑú ÏàòÏã†Ìïú Î≤ÑÌçºÎ•º Î∞òÌôòÌïòÎäî Ìï®Ïàò\n",
    "def recvall(sock, count):\n",
    "    # Î∞îÏù¥Ìä∏ Î¨∏ÏûêÏó¥\n",
    "    buf = b''\n",
    "    while count:\n",
    "        newbuf = sock.recv(count)\n",
    "        if not newbuf: return None\n",
    "        buf += newbuf\n",
    "        count -= len(newbuf)\n",
    "    return buf\n",
    "\n",
    "\n",
    "HOST = '141.223.140.32'\n",
    "PORT = 7778\n",
    "\n",
    "# TCP ÏÇ¨Ïö©\n",
    "s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "print('Socket created')\n",
    "\n",
    "# ÏÑúÎ≤ÑÏùò ÏïÑÏù¥ÌîºÏôÄ Ìè¨Ìä∏Î≤àÌò∏ ÏßÄÏ†ï\n",
    "s.bind((HOST, PORT))\n",
    "print('Socket bind complete')\n",
    "# ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Ïùò Ï†ëÏÜçÏùÑ Í∏∞Îã§Î¶∞Îã§. (ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Í≤∞ÏùÑ 10Í∞úÍπåÏßÄ Î∞õÎäîÎã§)\n",
    "s.listen(10)\n",
    "print('Socket now listening')\n",
    "\n",
    "# Ïó∞Í≤∞, connÏóêÎäî ÏÜåÏºì Í∞ùÏ≤¥, addrÏùÄ ÏÜåÏºìÏóê Î∞îÏù∏Îìú Îêú Ï£ºÏÜå\n",
    "conn, addr = s.accept()\n",
    "img_counter = 0\n",
    "frame_set = []\n",
    "start_time = time.time()\n",
    "temp=1\n",
    "i=0\n",
    "\n",
    "    \n",
    "while True:\n",
    "    length = recvall(conn, 16)\n",
    "    stringData = recvall(conn, int(length))\n",
    "    data = np.fromstring(stringData, dtype='uint8')\n",
    "    \n",
    "    frame = cv2.imdecode(data, cv2.IMREAD_COLOR)\n",
    "    if time.time() - start_time >= 5: \n",
    "        #img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "        imgname= './test/IMGtest'+str(temp)+'.jpg'\n",
    "        cv2.imwrite(imgname, frame)\n",
    "        #print(\"{} written!\".format(img_counter))\n",
    "        start_time = time.time()\n",
    "        temp +=1\n",
    "        a,b,c = pathSet('/home/piai/MyYolov5/test/')\n",
    "        predict(weights= c, imgsz=416, conf_thres=0.5, source =a[i], temp1=b[i])\n",
    "        i+=1\n",
    "    img_counter += 1\n",
    "    \n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socket created\n",
      "Socket bind complete\n",
      "Socket now listening\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e399d79d4525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Ïó∞Í≤∞, connÏóêÎäî ÏÜåÏºì Í∞ùÏ≤¥, addrÏùÄ ÏÜåÏºìÏóê Î∞îÏù∏Îìú Îêú Ï£ºÏÜå\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mframe_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/yoloTest/lib/python3.8/socket.py\u001b[0m in \u001b[0;36maccept\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mFor\u001b[0m \u001b[0mIP\u001b[0m \u001b[0msockets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maddress\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhostaddr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \"\"\"\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Issue #7995: if no default timeout is set and the listening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filename_to_list(folder_path):\n",
    "    targerdir = r\"/home/piai/MyYolov5/test/\"\n",
    "    files = os.listdir(folder_path)\n",
    "    result = []\n",
    "\n",
    "    for i in files:\n",
    "        if os.path.isdir(targerdir + r\"\\\\\" + i):\n",
    "            pass\n",
    "        else :\n",
    "            if i.count(\".\") == 1 :\n",
    "                V = i.split(\".\")\n",
    "                result.append(V[0])                  \n",
    "    return result         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/piai/MyYolov5/test/IMGtest1.jpg', '/home/piai/MyYolov5/test/IMGtest2.jpg', '/home/piai/MyYolov5/test/IMGtest3.jpg', '/home/piai/MyYolov5/test/IMGtest4.jpg', '/home/piai/MyYolov5/test/IMGtest5.jpg', '/home/piai/MyYolov5/test/IMGtest6.jpg', '/home/piai/MyYolov5/test/IMGtest7.jpg', '/home/piai/MyYolov5/test/IMGtest8.jpg']\n",
      "['result_IMGtest1', 'result_IMGtest2', 'result_IMGtest3', 'result_IMGtest4', 'result_IMGtest5', 'result_IMGtest6', 'result_IMGtest7', 'result_IMGtest8']\n",
      "/home/piai/MyYolov5/our_model.pt\n"
     ]
    }
   ],
   "source": [
    "def pathSet(filepath):\n",
    "    modelpath = '/home/piai/MyYolov5/our_model.pt'\n",
    "    test_img_path = glob(filepath+'*.jpg')\n",
    "    test_img_path.sort()\n",
    "    test_img_name = filename_to_list(filepath)\n",
    "    test_img_name.sort()\n",
    "    save_name=[]\n",
    "    for i in range(len(test_img_name)):\n",
    "        save_name.append('result_'+test_img_name[i])\n",
    "        \n",
    "    return test_img_path, save_name , modelpath\n",
    "    \n",
    "a,b,c = pathSet('/home/piai/MyYolov5/test/')\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "/home/piai/anaconda3/envs/yoloTest/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest1.jpg: 320x416 Done. (0.007s)\n",
      "Results saved to results\n",
      "Done. (0.024s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest2.jpg: 320x416 Done. (0.005s)\n",
      "Results saved to results\n",
      "Done. (0.019s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest3.jpg: 320x416 Done. (0.007s)\n",
      "Results saved to results\n",
      "Done. (0.021s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest4.jpg: 320x416 Done. (0.005s)\n",
      "Results saved to results\n",
      "Done. (0.020s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest5.jpg: 320x416 Done. (0.006s)\n",
      "Results saved to results\n",
      "Done. (0.021s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest6.jpg: 320x416 1 saewookkang, 1 zzappagaetti, Done. (0.006s)\n",
      "Results saved to results\n",
      "Done. (0.024s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ beecfe7 torch 1.9.0+cu102 CUDA:0 (GeForce RTX 2080, 7982.3125MB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/piai/MyYolov5/test/IMGtest7.jpg: 320x416 1 TWIX, 1 saewookkang, 1 zzappagaetti, Done. (0.008s)\n",
      "Results saved to results\n",
      "Done. (0.026s)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b31124ca49d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mcapture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(0)\n",
    "capture.set(3, 640)\n",
    "capture.set(4, 480)\n",
    "img_counter = 0\n",
    "frame_set = []\n",
    "start_time = time.time()\n",
    "temp=1\n",
    "i=0\n",
    "\n",
    "    \n",
    "while True:\n",
    "    \n",
    "    ret, frame = capture.read()\n",
    "    cv2.imshow('frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "    if time.time() - start_time >= 5: \n",
    "        #img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "        imgname= './test/IMGtest'+str(temp)+'.jpg'\n",
    "        cv2.imwrite(imgname, frame)\n",
    "        #print(\"{} written!\".format(img_counter))\n",
    "        start_time = time.time()\n",
    "        temp +=1\n",
    "        a,b,c = pathSet('/home/piai/MyYolov5/test/')\n",
    "        predict(weights= c, imgsz=416, conf_thres=0.5, source =a[i], temp1=b[i])\n",
    "        i+=1\n",
    "    img_counter += 1\n",
    "\n",
    "capture.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict(weights='yolov5s.pt',  # model.pt path(s)\n",
    "           source='data/images',  # file/dir/URL/glob, 0 for webcam\n",
    "           imgsz=640,  # inference size (pixels)\n",
    "           conf_thres=0.25,  # confidence threshold\n",
    "           iou_thres=0.45,  # NMS IOU threshold\n",
    "           max_det=1000,  # maximum detections per image\n",
    "           device='',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "           view_img=False,  # show results\n",
    "           save_txt=False,  # save results to *.txt\n",
    "           save_conf=False,  # save confidences in --save-txt labels\n",
    "           save_crop=False,  # save cropped prediction boxes\n",
    "           nosave=False,  # do not save images/videos\n",
    "           classes=None,  # filter by class: --class 0, or --class 0 2 3\n",
    "           agnostic_nms=False,  # class-agnostic NMS\n",
    "           augment=False,  # augmented inference\n",
    "           update=False,  # update all models\n",
    "           project='results',  # save results to project/name\n",
    "           name='exp',  # save results to project/name\n",
    "           exist_ok=False,  # existing project/name ok, do not increment\n",
    "           line_thickness=3,  # bounding box thickness (pixels)\n",
    "           hide_labels=False,  # hide labels\n",
    "           hide_conf=False,  # hide confidences\n",
    "           half=False,  # use FP16 half-precision inference\n",
    "           temp1=''\n",
    "           ):\n",
    "    save_img = not nosave and not source.endswith('.txt')  # save inference images\n",
    "    webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "        ('rtsp://', 'rtmp://', 'http://', 'https://'))\n",
    "\n",
    "    # Directories\n",
    "    #save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n",
    "    save_dir = Path(project)\n",
    "    (save_dir / 'labels' if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n",
    "\n",
    "    # Initialize\n",
    "    set_logging()\n",
    "    device = select_device(device)\n",
    "    half &= device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "    # Load model\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    stride = int(model.stride.max())  # model stride\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n",
    "    if half:\n",
    "        model.half()  # to FP16\n",
    "\n",
    "    # Second-stage classifier\n",
    "    classify = False\n",
    "    if classify:\n",
    "        modelc = load_classifier(name='resnet50', n=2)  # initialize\n",
    "        modelc.load_state_dict(torch.load('resnet50.pt', map_location=device)['model']).to(device).eval()\n",
    "\n",
    "    # Set Dataloader\n",
    "    vid_path, vid_writer = None, None\n",
    "    if webcam:\n",
    "        view_img = check_imshow()\n",
    "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "        dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "    else:\n",
    "        dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "    # Run inference\n",
    "    if device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "    t0 = time.time()\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        t1 = time_synchronized()\n",
    "        pred = model(img, augment=augment)[0]\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "        t2 = time_synchronized()\n",
    "\n",
    "        # Apply Classifier\n",
    "        if classify:\n",
    "            pred = apply_classifier(pred, modelc, img, im0s)\n",
    "\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            if webcam:  # batch_size >= 1\n",
    "                p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\n",
    "            else:\n",
    "                p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "            p = Path(p)  # to Path\n",
    "            #save_path = str(save_dir / p.name)  # img.jpg\n",
    "            save_path = str(save_dir / p.name)  # img.jpg\n",
    "            #save_path'/home/piai/testYolo'\n",
    "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
    "            s += '%gx%g ' % img.shape[2:]  # print string\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    if save_txt:  # Write to file\n",
    "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                        with open(txt_path + '.txt', 'a') as f:\n",
    "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
    "                        c = int(cls)  # integer class\n",
    "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                        plot_one_box(xyxy, im0, label=label, color=colors(c, True), line_thickness=line_thickness)\n",
    "                        if save_crop:\n",
    "                            save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
    "\n",
    "            # Print time (inference + NMS)\n",
    "            print(f'{s}Done. ({t2 - t1:.3f}s)')\n",
    "\n",
    "            # Stream results\n",
    "            if view_img:\n",
    "                cv2.imshow(str(p), im0)\n",
    "                cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            #imgname= '/home/piai/MyYolov5/ddd.jpg'\n",
    "            #global temp1\n",
    "            save_dir1 = str(save_dir) + '/'\n",
    "            imgname= save_dir1+temp1+'.jpg'\n",
    "            if save_img:\n",
    "                if dataset.mode == 'image':\n",
    "                    cv2.imwrite(imgname, im0)\n",
    "                    \n",
    "                else:  # 'video' or 'stream'\n",
    "                    if vid_path != save_path:  # new video\n",
    "                        vid_path = save_path\n",
    "                        if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                            vid_writer.release()  # release previous video writer\n",
    "                        if vid_cap:  # video\n",
    "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                        else:  # stream\n",
    "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                            save_path += '.mp4'\n",
    "                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                    vid_writer.write(im0)\n",
    "\n",
    "    if save_txt or save_img:\n",
    "        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "        print(f\"Results saved to {save_dir}{s}\")\n",
    "\n",
    "    if update:\n",
    "        strip_optimizer(weights)  # update model (to fix SourceChangeWarning)\n",
    "\n",
    "    print(f'Done. ({time.time() - t0:.3f}s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloTest",
   "language": "python",
   "name": "yolotest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
